<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SeaLLMs - Large Language Models for Southeast Asia">
  <meta name="keywords" content="SeaLLM, SeaLMMM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SeaLLMs - Large Language Models for Southeast Asia</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon-32x32.png"> -->
  <link rel="icon" href="./static/images/seal_logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>



  

</head>
<body>

<!-- @article{damonlpsg2023seallm,
  author = {Xuan-Phi Nguyen*, Wenxuan Zhang*, Xin Li*, Mahani Aljunied*,
            Zhiqiang Hu, Chenhui Shen^, Yew Ken Chia^, Xingxuan Li, Jianyu Wang,
            Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang,
            Chaoqun Liu, Hang Zhang, Lidong Bing},
  title = {SeaLLMs - Large Language Models for Southeast Asia},
  year = 2023,
  Eprint = {arXiv:2312.00738},
} -->

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <img src="./static/images/seal_logo.png" alt="" style="height: 2.5em;">
            <br>
            SeaLLMs - Large Language Models for Southeast Asia
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <!-- <a href="https://nxphi47.github.io">Xuan-Phi Nguyen*</a><sup>1</sup>,</span> -->
              <a href="https://nxphi47.github.io">Xuan-Phi Nguyen</a>,
            </span>
            <span class="author-block">
              <a href="https://isakzhang.github.io">Wenxuan Zhang</a>,
            </span>
            <span class="author-block">
              <a href="https://lixin4ever.github.io">Xin Li</a>,
            </span>
            <span class="author-block">
              <a href="">Mahani Aljunied</a>,
            </span>
            <span class="author-block">
              <a href="https://wwxu21.github.io">Weiwen Xu</a>,
            </span>
            <span class="author-block">
              <a href="https://kenchan0226.github.io">Hou Pong Chan</a>,
            </span>
            <span class="author-block">
              <a href="https://hzq950419.github.io/HomePage/">Zhiqiang Hu</a>,
            </span>
            <span class="author-block">
              <a href="">Chenhui Shen</a>,
            </span>
            <span class="author-block">
              <a href="https://chiayewken.com/">Yew Ken Chia</a>,
            </span>
            <span class="author-block">
              <a href="https://lixin4ever.github.io">Xingxuan Li</a>,
            </span>
            <span class="author-block">
              <a href="">Jianyu Wang</a>,
            </span>
            <span class="author-block">
              <a href="">Qingyu Tan</a>,
            </span>
            <span class="author-block">
              <a href="">Liying Cheng</a>,
            </span>
            <span class="author-block">
              <a href="">Guanzheng Chen</a>,
            </span>
            <span class="author-block">
              <a href="">Yue Deng</a>,
            </span>
            <span class="author-block">
              <a href="">Sen Yang</a>,
            </span>
            <span class="author-block">
              <a href="">Chaoqun Liu</a>,
            </span>
            <span class="author-block">
              <a href="">Hang Zhang</a> &
            </span>
            <span class="author-block">
              <a href="https://lidongbing.github.io">Lidong Bing*</a> (Corresponding Author)
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">DAMO Academy, Alibaba Group</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.00738"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="fas fa-file-pdf"></i> -->
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Demo Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/spaces/SeaLLMs/SeaLLM-Chat"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      ðŸ¤—
                  </span>
                  <span>DEMO</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/DAMO-NLP-SG/SeaLLMs"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Github</span>
                  </a>
              </span>
              <!-- Models Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/SeaLLMs/SeaLLM3-7B-Chat"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      ðŸ¤—
                  </span>
                  <span>Models</span>
                  </a>
                </span>
              <!-- darasets Link. -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/datasets/SeaLLMs/Sea-bench"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
                </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h4 class="subtitle has-text-centered">
        ðŸ”¥<span style="color: #ff3860">[NEW!]</span> 
        <a href="https://huggingface.co/SeaLLMs/SeaLLM3-7B-Chat">SeaLLMs-v3</a> is released with SoTA performance in various tasks and specifically enhanced to be more trustworthy.
        <br>
        <!--
        ðŸ”¥<span style="color: #ff3860">[HOT!]</span> 
        <a href="https://huggingface.co/SeaLLMs/SeaLMMM-7B-v0.1">SeaLMMM-7B-v0.1</a> is introduced with <b>Multimodal</b> Multilingual capabilities in SEA languages.
        -->
      </h4>
      <script
      type="module"
      src="https://gradio.s3-us-west-2.amazonaws.com/4.26.0/gradio.js">
      </script>
      <gradio-app src="https://seallms-seallm-7b-v2-5-simple.hf.space"></gradio-app>

    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce SeaLLMs-v3, the latest series of the SeaLLMs (Large Language Models for Southeast Asian languages) family. It achieves state-of-the-art performance among models with similar sizes, excelling across a diverse array of tasks such as world knowledge, mathematical reasoning, translation, and instruction following. In the meantime, it was specifically enhanced to be more trustworthy, exhibiting reduced hallucination and providing safe responses, particularly in queries closed related to Southeast Asian culture.
          </p>
          <p>
            <a href="https://huggingface.co/collections/SeaLLMs/seallms-65be16f92e67686440ae29f3">SeaLLMs</a> is a continuously iterated and improved series of language models
            that specifically focuses on Southeast Asian (SEA) languages. SeaLLMs are typically continue-pretrained and fine-tuned from strong English models to build outstanding
            capabilities in SEA languages without degrading performances in high-resource languages. SeaLLMs are built with focus in prioritizing local cultural and legal norms, customs, 
            stylistic preferences, as well as cost-effectiveness
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<!-- introduction -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Introduction</h2>
  </div>
</section> -->

<!-- <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4">SeaLLM-7B-v2.5 DEMO</h2>
    <gradio-app src="https://seallms-seallm-7b-v2-5-simple.hf.space"></gradio-app>
  </div>    
</section> -->


<!-- Evaluation -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- <h2 class="title is-3">Evaluation</h2> -->

    <h2 class="title is-4">Multilingual World Knowledge - M3Exam</h2>
    <div class="content has-text-justified">
      <p><a href="https://arxiv.org/abs/2306.05179">M3Exam</a> consists of local exam questions collected from each country. It reflects the model's world knowledge (e.g., with language or social science subjects) and reasoning abilities (e.g., with mathematics or natural science subjects).</p>
      
      <div class="table-container">
        <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Model</th>
              <th>en</th>
              <th>zh</th>
              <th>id</th>
              <th>th</th>
              <th>vi</th>
              <th>avg</th>
              <th>avg_sea</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Sailor-7B-Chat</td>
              <td>0.66</td>
              <td>0.652</td>
              <td>0.475</td>
              <td>0.462</td>
              <td>0.513</td>
              <td>0.552</td>
              <td>0.483</td>
            </tr>
            <tr>
              <td>gemma-7b</td>
              <td>0.732</td>
              <td>0.519</td>
              <td>0.475</td>
              <td>0.46</td>
              <td>0.594</td>
              <td>0.556</td>
              <td>0.510</td>
            </tr>
            <tr>
              <td>SeaLLM-7B-v2.5</td>
              <td>0.758</td>
              <td>0.581</td>
              <td>0.499</td>
              <td>0.502</td>
              <td>0.622</td>
              <td>0.592</td>
              <td>0.541</td>
            </tr>
            <tr>
              <td>Qwen2-7B</td>
              <td>0.815</td>
              <td>0.874</td>
              <td>0.53</td>
              <td>0.479</td>
              <td>0.628</td>
              <td>0.665</td>
              <td>0.546</td>
            </tr>
            <tr>
              <td>Qwen2-7B-Instruct</td>
              <td>0.809</td>
              <td>0.88</td>
              <td>0.558</td>
              <td>0.555</td>
              <td>0.624</td>
              <td>0.685</td>
              <td>0.579</td>
            </tr>
            <tr>
              <td>Sailor-14B</td>
              <td>0.748</td>
              <td>0.84</td>
              <td>0.536</td>
              <td>0.528</td>
              <td>0.621</td>
              <td>0.655</td>
              <td>0.562</td>
            </tr>
            <tr>
              <td>Sailor-14B-Chat</td>
              <td>0.749</td>
              <td>0.843</td>
              <td>0.553</td>
              <td>0.566</td>
              <td>0.637</td>
              <td>0.67</td>
              <td>0.585</td>
            </tr>
            <tr>
              <td>SeaLLMs-v3-7B</td>
              <td>0.814</td>
              <td>0.866</td>
              <td>0.549</td>
              <td>0.52</td>
              <td>0.628</td>
              <td>0.675</td>
              <td>0.566</td>
            </tr>
            <tr>
              <td>SeaLLMs-v3-7B-Chat</td>
              <td>0.809</td>
              <td>0.874</td>
              <td>0.558</td>
              <td>0.569</td>
              <td>0.649</td>
              <td>0.692</td>
              <td>0.592</td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>

    <!-- seaexam -->
    <!--
    <h2 class="title is-4">SeaExam Leaderboard</h2>
    <div class="content has-text-justified">
      <p>
        According to the <a href="https://huggingface.co/spaces/SeaLLMs/SeaExam_leaderboard">SeaExam leaderboard</a>, which evaluates model performance through human-exam style questions in Southeast Asian languages, the latest SeaLLMs-v2.5 is ranked at the top among open-source models of similar size.
        <gradio-app src="https://seallms-seaexam-leaderboard.hf.space"></gradio-app>
      </p>
    </div>
    -->

    <!-- SeaBench -->
    <h2 class="title is-4">Multilingual Instruction-following Capability - SeaBench</h2>
    <div class="content has-text-justified">
      <p>SeaBench consists of multi-turn human instructions spanning various task types. It evaluates chat-based models on their ability to follow human instructions in both single and multi-turn settings and assesses their performance across different task types. The dataset and corresponding evaluation code will be released soon!</p>
      
      <div class="table-container">
        <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Model</th>
              <th>id<br>turn1</th>
              <th>id<br>turn2</th>
              <th>id<br>avg</th>
              <th>th<br>turn1</th>
              <th>th<br>turn2</th>
              <th>th<br>avg</th>
              <th>vi<br>turn1</th>
              <th>vi<br>turn2</th>
              <th>vi<br>avg</th>
              <th>avg</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Qwen2-7B-Instruct</td>
              <td>5.93</td>
              <td>5.84</td>
              <td>5.89</td>
              <td>5.47</td>
              <td>5.20</td>
              <td>5.34</td>
              <td>6.17</td>
              <td>5.60</td>
              <td>5.89</td>
              <td>5.70</td>
            </tr>
            <tr>
              <td>SeaLLM-7B-v2.5</td>
              <td>6.27</td>
              <td>4.96</td>
              <td>5.62</td>
              <td>5.79</td>
              <td>3.82</td>
              <td>4.81</td>
              <td>6.02</td>
              <td>4.02</td>
              <td>5.02</td>
              <td>5.15</td>
            </tr>
            <tr>
              <td>Sailor-14B-Chat</td>
              <td>5.26</td>
              <td>5.53</td>
              <td>5.40</td>
              <td>4.62</td>
              <td>4.36</td>
              <td>4.49</td>
              <td>5.31</td>
              <td>4.74</td>
              <td>5.03</td>
              <td>4.97</td>
            </tr>
            <tr>
              <td>Sailor-7B-Chat</td>
              <td>4.60</td>
              <td>4.04</td>
              <td>4.32</td>
              <td>3.94</td>
              <td>3.17</td>
              <td>3.56</td>
              <td>4.82</td>
              <td>3.62</td>
              <td>4.22</td>
              <td>4.03</td>
            </tr>
            <tr>
              <td>SeaLLMs-v3-7B-Chat</td>
              <td>6.73</td>
              <td>6.59</td>
              <td>6.66</td>
              <td>6.48</td>
              <td>5.90</td>
              <td>6.19</td>
              <td>6.34</td>
              <td>5.79</td>
              <td>6.07</td>
              <td>6.31</td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>

    <h2 class="title is-4">Multilingual Math</h2>
    <div class="content has-text-justified">
      <p>We evaluate the multilingual math capability using the MGSM dataset. MGSM originally contains Chinese and Thai testing sets only, we use Google Translate to translate the same English questions into other SEA languages. Note that we adopt the tradition of each country to represent the number, e.g., in Indonesian and Vietnamese, dots are used as thousands separators and commas as decimal separators, the opposite of the English system.</p>
      
      <div class="table-container">
        <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>MGSM</th>
              <th>en</th>
              <th>id</th>
              <th>ms</th>
              <th>th</th>
              <th>vi</th>
              <th>zh</th>
              <th>avg</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Sailor-7B-Chat</td>
              <td>33.6</td>
              <td>22.4</td>
              <td>22.4</td>
              <td>21.6</td>
              <td>25.2</td>
              <td>29.2</td>
              <td>25.7</td>
            </tr>
            <tr>
              <td>Meta-Llama-3-8B-Instruct</td>
              <td>77.6</td>
              <td>48</td>
              <td>57.6</td>
              <td>56</td>
              <td>46.8</td>
              <td>58.8</td>
              <td>57.5</td>
            </tr>
            <tr>
              <td>glm-4-9b-chat</td>
              <td>72.8</td>
              <td>53.6</td>
              <td>53.6</td>
              <td>34.8</td>
              <td>52.4</td>
              <td>70.8</td>
              <td>56.3</td>
            </tr>
            <tr>
              <td>Qwen1.5-7B-Chat</td>
              <td>64</td>
              <td>34.4</td>
              <td>38.4</td>
              <td>25.2</td>
              <td>36</td>
              <td>53.6</td>
              <td>41.9</td>
            </tr>
            <tr>
              <td>Qwen2-7B-instruct</td>
              <td>82</td>
              <td>66.4</td>
              <td>62.4</td>
              <td>58.4</td>
              <td>64.4</td>
              <td>76.8</td>
              <td>68.4</td>
            </tr>
            <tr>
              <td>aya-23-8B</td>
              <td>28.8</td>
              <td>16.4</td>
              <td>14.4</td>
              <td>2</td>
              <td>16</td>
              <td>12.8</td>
              <td>15.1</td>
            </tr>
            <tr>
              <td>gemma-1.1-7b-it</td>
              <td>58.8</td>
              <td>32.4</td>
              <td>34.8</td>
              <td>31.2</td>
              <td>39.6</td>
              <td>35.2</td>
              <td>38.7</td>
            </tr>
            <tr>
              <td>SeaLLM-7B-v2.5</td>
              <td>79.6</td>
              <td>69.2</td>
              <td>70.8</td>
              <td>61.2</td>
              <td>66.8</td>
              <td>62.4</td>
              <td>68.3</td>
            </tr>
            <tr>
              <td>SeaLLMs-v3-7B-Chat</td>
              <td>74.8</td>
              <td>71.2</td>
              <td>70.8</td>
              <td>71.2</td>
              <td>71.2</td>
              <td>79.6</td>
              <td>73.1</td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>

    <h2 class="title is-4">Translation</h2>
    <div class="content has-text-justified">
      <p>We use the test sets from Flores-200 for evaluation and report the zero-shot chrF scores for translations between every pair of languages. Each row in the table below presents the average results of translating from various source languages into the target languages. The last column displays the overall average results of translating from any language to any other language for each model.</p>
      
      <div class="table-container">
        <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Model</th>
              <th>en</th>
              <th>id</th>
              <th>jv</th>
              <th>km</th>
              <th>lo</th>
              <th>ms</th>
              <th>my</th>
              <th>ta</th>
              <th>th</th>
              <th>tl</th>
              <th>vi</th>
              <th>zh</th>
              <th>avg</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Meta-Llama-3-8B-Instruct</td>
              <td>51.54</td>
              <td>49.03</td>
              <td>22.46</td>
              <td>15.34</td>
              <td>5.42</td>
              <td>46.72</td>
              <td>21.24</td>
              <td>32.09</td>
              <td>35.75</td>
              <td>40.80</td>
              <td>39.31</td>
              <td>14.87</td>
              <td>31.22</td>
            </tr>
            <tr>
              <td>Qwen2-7B-Instruct</td>
              <td>50.36</td>
              <td>47.55</td>
              <td>29.36</td>
              <td>19.26</td>
              <td>11.06</td>
              <td>42.43</td>
              <td>19.33</td>
              <td>20.04</td>
              <td>36.07</td>
              <td>37.91</td>
              <td>39.63</td>
              <td>22.87</td>
              <td>31.32</td>
            </tr>
            <tr>
              <td>Sailor-7B-Chat</td>
              <td>49.40</td>
              <td>49.78</td>
              <td>28.33</td>
              <td>2.68</td>
              <td>6.85</td>
              <td>47.75</td>
              <td>5.35</td>
              <td>18.23</td>
              <td>38.92</td>
              <td>29.00</td>
              <td>41.76</td>
              <td>20.87</td>
              <td>28.24</td>
            </tr>
            <tr>
              <td>SeaLLM-7B-v2.5</td>
              <td>55.09</td>
              <td>53.71</td>
              <td>18.13</td>
              <td>18.09</td>
              <td>15.53</td>
              <td>51.33</td>
              <td>19.71</td>
              <td>26.10</td>
              <td>40.55</td>
              <td>45.58</td>
              <td>44.56</td>
              <td>24.18</td>
              <td>34.38</td>
            </tr>
            <tr>
              <td>SeaLLMs-v3-7B-Chat</td>
              <td>54.68</td>
              <td>52.52</td>
              <td>29.86</td>
              <td>27.30</td>
              <td>26.34</td>
              <td>45.04</td>
              <td>21.54</td>
              <td>31.93</td>
              <td>41.52</td>
              <td>38.51</td>
              <td>43.78</td>
              <td>26.10</td>
              <td>36.52</td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>

    <h2 class="title is-4">Hallucination</h2>
    <div class="content has-text-justified">
      <p>Performance of whether a model can refuse questions about the non-existing entity. The following is the F1 score. We use refusal as the positive label. Our test set consists of ~1k test samples per language. Each unanswerable question is generated by GPT4o. The ratio of answerable and unanswerable questions are 1:1. We define keywords to automatically detect whether a model-generated response is a refusal response.</p>
      
      <div class="table-container">
        <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Refusal-F1 Scores</th>
              <th>en</th>
              <th>zh</th>
              <th>vi</th>
              <th>th</th>
              <th>id</th>
              <th>avg</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Qwen1.5-7B-Instruct</td>
              <td>53.85</td>
              <td>51.70</td>
              <td>52.85</td>
              <td>35.50</td>
              <td>58.40</td>
              <td>50.46</td>
            </tr>
            <tr>
              <td>Qwen2-7B-Instruct</td>
              <td>58.79</td>
              <td>33.08</td>
              <td>56.21</td>
              <td>44.60</td>
              <td>55.98</td>
              <td>49.732</td>
            </tr>
            <tr>
              <td>SeaLLM-7B-v2.5</td>
              <td>12.90</td>
              <td>0.77</td>
              <td>2.45</td>
              <td>19.42</td>
              <td>0.78</td>
              <td>7.26</td>
            </tr>
            <tr>
              <td>Sailor-7B-Chat</td>
              <td>33.49</td>
              <td>18.82</td>
              <td>5.19</td>
              <td>9.68</td>
              <td>16.42</td>
              <td>16.72</td>
            </tr>
            <tr>
              <td>glm-4-9b-chat</td>
              <td>44.48</td>
              <td>37.89</td>
              <td>18.66</td>
              <td>4.27</td>
              <td>1.97</td>
              <td>21.45</td>
            </tr>
            <tr>
              <td>aya-23-8B</td>
              <td>6.38</td>
              <td>0.79</td>
              <td>2.83</td>
              <td>1.98</td>
              <td>14.80</td>
              <td>5.36</td>
            </tr>
            <tr>
              <td>Llama-3-8B-Instruct</td>
              <td>72.08</td>
              <td>0.00</td>
              <td>1.23</td>
              <td>0.80</td>
              <td>3.91</td>
              <td>15.60</td>
            </tr>
            <tr>
              <td>gemma-1.1-7b-it</td>
              <td>52.39</td>
              <td>27.74</td>
              <td>23.96</td>
              <td>22.97</td>
              <td>31.72</td>
              <td>31.76</td>
            </tr>
            <tr>
              <td>SeaLLMs-v3-7B-Chat</td>
              <td>71.36</td>
              <td>78.39</td>
              <td>77.93</td>
              <td>61.31</td>
              <td>68.95</td>
              <td>71.588</td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>

    <h2 class="title is-4">Safety</h2>
    <div class="content has-text-justified">
      <p>Multijaildataset consists of harmful prompts in multiple languages. We take those relevant prompts in SEA languages here and report their safe rate (the higher the better).</p>
      
      <div class="table-container">
        <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Model</th>
              <th>en</th>
              <th>jv</th>
              <th>th</th>
              <th>vi</th>
              <th>zh</th>
              <th>avg</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Qwen2-7B-Instruct</td>
              <td>0.8857</td>
              <td>0.4381</td>
              <td>0.6381</td>
              <td>0.7302</td>
              <td>0.873</td>
              <td>0.713</td>
            </tr>
            <tr>
              <td>Sailor-7B-Chat</td>
              <td>0.7873</td>
              <td>0.5492</td>
              <td>0.6222</td>
              <td>0.6762</td>
              <td>0.7619</td>
              <td>0.6794</td>
            </tr>
            <tr>
              <td>Meta-Llama-3-8B-Instruct</td>
              <td>0.8825</td>
              <td>0.2635</td>
              <td>0.7111</td>
              <td>0.6984</td>
              <td>0.7714</td>
              <td>0.6654</td>
            </tr>
            <tr>
              <td>Sailor-14B-Chat</td>
              <td>0.8698</td>
              <td>0.3048</td>
              <td>0.5365</td>
              <td>0.6095</td>
              <td>0.727</td>
              <td>0.6095</td>
            </tr>
            <tr>
              <td>glm-4-9b-chat</td>
              <td>0.7714</td>
              <td>0.2127</td>
              <td>0.3016</td>
              <td>0.6063</td>
              <td>0.7492</td>
              <td>0.52824</td>
            </tr>
            <tr>
              <td>SeaLLMs-v3-7B-Chat</td>
              <td>0.8889</td>
              <td>0.6000</td>
              <td>0.7333</td>
              <td>0.8381</td>
              <td>0.927</td>
              <td>0.7975</td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>
  </div>
</section>


<!-- model information -->



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Related Links</h2>
    <div class="content has-text-justified">
      <p>
        <a href="https://huggingface.co/SeaLLMs/SeaLLM3-7B-Chat">SeaLLMs-v3</a> was released in July 2024. It achieves SOTA performance of diverse tasks while specifically enhanced to be more trustworthy, exhibiting reduced hallucination and providing safe response.
      </p>

      <p>
        <a href="https://huggingface.co/SeaLLMs/SeaLLM-7B-v2">SeaLLM-7B-v2.5</a> was released in April 2024. It possesses outstanding abilities in world knowledge and math reasoning in both English and SEA languages.
      </p>
      <p>
        <a href="https://huggingface.co/SeaLLMs/SeaLLM-7B-v2">SeaLLM-7B-v2</a> was released in Feb 2024. It possesses outstanding abilities in math and commonsense reasoning in Sea languages.
      </p>
      <p>
        <a href="https://huggingface.co/SeaLLMs/SeaLLM-7B-v1">SeaLLM-7B-v1</a> was released in Nov 2023. It was the first release of SeaLLMs model family, and the first LLM built specifically for Southeast Asia.
      </p>
    </div>
  </div>

  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <p>We would like to express our special thanks to our professional and native linguists, Tantong Champaiboon, Nguyen Ngoc Yen Nhi and Tara Devina Putri, who helped build, evaluate, and fact-check our sampled pretraining and SFT dataset as well as evaluating our models across different aspects, especially safety.</p>
  </div>
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <p>
      If you find our project useful, we hope you would kindly star our repo and cite our work as follows.
      <br>
      Corresponding Author: <a href= "mailto: l.bing@alibaba-inc.com">l.bing@alibaba-inc.com</a>
    </p>
    <pre><code>@article{damonlpsg2023seallm,
  author = {Xuan-Phi Nguyen*, Wenxuan Zhang*, Xin Li*, Mahani Aljunied*, Weiwen Xu, Hou Pong Chan,
            Zhiqiang Hu, Chenhui Shen^, Yew Ken Chia^, Xingxuan Li, Jianyu Wang,
            Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang,
            Chaoqun Liu, Hang Zhang, Lidong Bing},
  title = {SeaLLMs - Large Language Models for Southeast Asia},
  year = {2024},
  booktitle = {ACL},
  url = {https://arxiv.org/pdf/2312.00738},
}
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2312.00738.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/DAMO-NLP-SG/SeaLLMs" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
